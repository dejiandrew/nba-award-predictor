{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1jeg0Rr1ri1In0iX6fpBRfbPG3EImHHwl",
      "authorship_tag": "ABX9TyOnjSMEtw28z76uEWYWK0NN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dejiandrew/nba-award-predictor/blob/deji/autogluon_automl_data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq23AVW1188W"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install h2o\n",
        "!pip install autogluon\n",
        "import wget\n",
        "import pandas as pd\n",
        "import h2o\n",
        "import joblib\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wget.download('https://storage.googleapis.com/nba_award_predictor/nba_data/features-overall-weekly.csv')\n",
        "features_overall_weekly_df = pd.read_csv('features-overall-weekly.csv')"
      ],
      "metadata": {
        "id": "Lox4Xah72B8o"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(features_overall_weekly_df.columns)"
      ],
      "metadata": {
        "id": "EVjP7X4QB4Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#features_overall_weekly_df = features_overall_weekly_df[features_overall_weekly_df['season'] >= 2021]\n",
        "features_overall_weekly_df = features_overall_weekly_df.drop(columns=['league_pts_mean', 'league_pts_std',\n",
        "       'league_ast_mean', 'league_ast_std', 'league_pm_mean', 'league_pm_std',\n",
        "       'z_s_pts', 'z_s_ast', 'z_s_pm'])\n",
        "features_overall_weekly_df"
      ],
      "metadata": {
        "id": "lrGiKH00cqMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that contain the answer. Also drop mvp, all-star, all-nba because they are from the future when we try to do inference during season\n",
        "leakage_cols = ['pow_player_id', 'player_of_the_week', 'pow_conference',\n",
        "                'all_star_this_season', 'mvp_this_season',\n",
        "                'all_nba_first_team_this_season', 'all_nba_second_team_this_season',\n",
        "                'all_nba_third_team_this_season']\n",
        "\n",
        "# Set aside identifier columns\n",
        "id_cols = ['player_id', 'full_name', 'team', 'season', 'week', 'week_start', 'conference']\n",
        "\n",
        "features_overall_weekly_df_clean = features_overall_weekly_df.drop(columns=leakage_cols + id_cols)\n",
        "df_encoded = pd.get_dummies(features_overall_weekly_df_clean, drop_first=True)\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "UTCvCKgD2OC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your grouped split data\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "X = df_encoded.drop(columns=['won_player_of_the_week'])\n",
        "y = df_encoded['won_player_of_the_week']\n",
        "identifiers = features_overall_weekly_df[id_cols]\n",
        "\n",
        "groups = (features_overall_weekly_df['season'].astype(str) + '_' +\n",
        "          features_overall_weekly_df['week'].astype(str) + '_' +\n",
        "          features_overall_weekly_df['conference'].astype(str))\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
        "\n",
        "# Create train and test CSVs\n",
        "train_data = X.iloc[train_idx].copy()\n",
        "train_data = train_data.replace([np.inf, -np.inf], np.nan)\n",
        "train_data['won_player_of_the_week'] = y.iloc[train_idx]\n",
        "\n",
        "test_data = X.iloc[test_idx].copy()\n",
        "test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
        "test_data['won_player_of_the_week'] = y.iloc[test_idx]\n"
      ],
      "metadata": {
        "id": "GCFbKF7kEHcx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# Train AutoGluon\n",
        "predictor = TabularPredictor(\n",
        "    label='won_player_of_the_week',\n",
        "    eval_metric='roc_auc',\n",
        "    path='./ag_models'\n",
        ").fit(\n",
        "    train_data=train_data,\n",
        "    time_limit=3600,\n",
        "    presets='best_quality',\n",
        "    num_gpus=1,\n",
        "    ag_args_fit={'num_gpus': 1},\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# Get predictions on test set\n",
        "test_pred_proba = predictor.predict_proba(test_data)\n",
        "\n",
        "# Create evaluation_df\n",
        "evaluation_df = identifiers.iloc[test_idx].copy().reset_index(drop=True)\n",
        "evaluation_df['actual_won'] = test_data['won_player_of_the_week'].values\n",
        "evaluation_df['pred_proba'] = test_pred_proba[1].values  # Probability of class 1\n",
        "\n",
        "evaluation_df.head()"
      ],
      "metadata": {
        "id": "Iff13LXy6z4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "\n",
        "# Total events\n",
        "query = \"\"\"\n",
        "SELECT COUNT(DISTINCT(season, conference, week))\n",
        "       AS total_season_conference_weeks_we_couldve_predicted\n",
        "FROM evaluation_df\n",
        "WHERE actual_won = 1\n",
        "\"\"\"\n",
        "\n",
        "total_season_conference_weeks_we_couldve_predicted = (\n",
        "    duckdb.query(query).df()['total_season_conference_weeks_we_couldve_predicted'].item()\n",
        ")\n",
        "\n",
        "print(\"Total POTW events in test set:\", total_season_conference_weeks_we_couldve_predicted)\n",
        "\n",
        "# Top-k accuracy\n",
        "for k in [1, 2, 3, 4, 5, 10]:\n",
        "    query = f\"\"\"\n",
        "    WITH Highest_Probabilities AS (\n",
        "      SELECT *,\n",
        "        RANK() OVER (\n",
        "           PARTITION BY season, conference, week\n",
        "           ORDER BY pred_proba DESC\n",
        "        ) AS rnk\n",
        "      FROM evaluation_df\n",
        "    ),\n",
        "    Actual_Winners AS (\n",
        "      SELECT * FROM evaluation_df WHERE actual_won = 1\n",
        "    )\n",
        "\n",
        "    SELECT *\n",
        "    FROM Highest_Probabilities\n",
        "    JOIN Actual_Winners\n",
        "      ON Highest_Probabilities.season = Actual_Winners.season\n",
        "     AND Highest_Probabilities.week = Actual_Winners.week\n",
        "     AND Highest_Probabilities.conference = Actual_Winners.conference\n",
        "    WHERE rnk <= {k}\n",
        "      AND Highest_Probabilities.pred_proba = Actual_Winners.pred_proba\n",
        "    \"\"\"\n",
        "\n",
        "    correct_predictions_df = duckdb.query(query).df()\n",
        "    total_correct_predictions = correct_predictions_df[\"actual_won\"].sum()\n",
        "\n",
        "    top_k_accuracy = 100 * total_correct_predictions / total_season_conference_weeks_we_couldve_predicted\n",
        "\n",
        "    print(f\"Top-{k} accuracy: {top_k_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "bsQtDYnBYdBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model name\n",
        "best_model_name = predictor.model_best\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "\n",
        "# See the leaderboard\n",
        "leaderboard = predictor.leaderboard(test_data)\n",
        "print(leaderboard)"
      ],
      "metadata": {
        "id": "TvHcVJRVs4H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance for the best model only\n",
        "feature_importance = predictor.feature_importance(test_data, model=predictor.model_best)\n",
        "\n",
        "# Show top 20\n",
        "print(f\"Top 20 Features for {predictor.model_best}:\")\n",
        "print(feature_importance.head(20))"
      ],
      "metadata": {
        "id": "l_HnosOutRQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Get feature names\n",
        "feature_names = list(train_data.drop(columns=['won_player_of_the_week']).columns)\n",
        "joblib.dump(feature_names, 'autogluon_model_features.pkl')\n",
        "\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "print(f\"Features saved to: ./autogluon_model_features.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvixGHs6vUNp",
        "outputId": "fe954826-2dfc-4516-b4cc-8560dc1ebc96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: WeightedEnsemble_L3\n",
            "Features saved to: ./autogluon_model_features_v2.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_potw_predictions(week_start: str):\n",
        "  \"\"\"\n",
        "  get_potw_predictions() does inference to predict POW.\n",
        "  week_start must be a Monday in 'yyyy-mm-dd' format\n",
        "  \"\"\"\n",
        "  import duckdb\n",
        "  import joblib\n",
        "  import wget\n",
        "  import pandas as pd\n",
        "  from autogluon.tabular import TabularPredictor\n",
        "\n",
        "  # features-overall-weekly-for-inference.csv is calculated in a separate script that runs within the daily production pipeline\n",
        "  # features-overall-weekly-for-inference.csv is needed to make predictions while the current week is in-progress (i.e. no POW announced yet)\n",
        "  wget.download('https://storage.googleapis.com/nba_award_predictor/nba_data/features-overall-weekly-for-inference.csv')\n",
        "\n",
        "  # Read in model (load the full predictor, not just the pkl)\n",
        "  predictor = TabularPredictor.load('./ag_models')\n",
        "  feature_names = joblib.load('autogluon_model_features.pkl')\n",
        "\n",
        "  # Read in player-week stats\n",
        "  overall_weekly_agg_df = pd.read_csv('features-overall-weekly-for-inference.csv')\n",
        "\n",
        "  # Remove unwanted features\n",
        "  overall_weekly_agg_df = overall_weekly_agg_df.drop(columns=['league_pts_mean', 'league_pts_std',\n",
        "       'league_ast_mean', 'league_ast_std', 'league_pm_mean', 'league_pm_std',\n",
        "       'z_s_pts', 'z_s_ast', 'z_s_pm'])\n",
        "\n",
        "  # Query all players who played for the input week\n",
        "  query = f\"\"\"\n",
        "  SELECT * FROM overall_weekly_agg_df\n",
        "  WHERE week_start = '{week_start}'\n",
        "  \"\"\"\n",
        "  df = duckdb.query(query).df()\n",
        "\n",
        "  #Keep player info separate\n",
        "  player_info = df[['player_id', 'full_name', 'conference', 'season', 'week']]\n",
        "  X = df[feature_names]\n",
        "\n",
        "  # Use predictor.predict_proba instead\n",
        "  probabilities = predictor.predict_proba(X)[1].values  # Get probability of class 1\n",
        "\n",
        "  # Attach probabilities to the player identities\n",
        "  results = player_info.copy()\n",
        "  results['probability'] = probabilities\n",
        "\n",
        "  # Now partition by season, week, and conference and return top 5 for each conference\n",
        "  query = f\"\"\"\n",
        "  WITH CTE AS (SELECT *,\n",
        "  RANK() OVER(PARTITION BY season, week, conference ORDER BY probability DESC) AS rank\n",
        "  FROM results\n",
        "  )\n",
        "\n",
        "  ,CTE2 AS (\n",
        "  SELECT * FROM CTE WHERE conference = 'East' AND rank <= 5\n",
        "  UNION ALL\n",
        "  SELECT * FROM CTE WHERE conference = 'West' AND rank <= 5\n",
        "\n",
        "  )\n",
        "  SELECT\n",
        "  '{week_start}' AS week_start\n",
        "  ,rank\n",
        "  ,full_name as name\n",
        "  ,conference\n",
        "  ,probability\n",
        "  FROM CTE2\n",
        "  \"\"\"\n",
        "\n",
        "  top_five_per_conference = duckdb.query(query).df()\n",
        "\n",
        "  return top_five_per_conference\n",
        "\n",
        "get_potw_predictions('2025-12-15')"
      ],
      "metadata": {
        "id": "6g_-8qrGPixJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "def upload_directory_to_gcs(local_path, bucket_name, gcs_path, credentials_path):\n",
        "    \"\"\"Upload a directory to GCS preserving structure\"\"\"\n",
        "    storage_client = storage.Client.from_service_account_json(credentials_path)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    for root, dirs, files in os.walk(local_path):\n",
        "        for file in files:\n",
        "            local_file = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(local_file, local_path)\n",
        "            blob_path = os.path.join(gcs_path, relative_path)\n",
        "\n",
        "            blob = bucket.blob(blob_path)\n",
        "            blob.upload_from_filename(local_file)\n",
        "            print(f\"Uploaded {local_file} to gs://{bucket_name}/{blob_path}\")\n",
        "\n",
        "# Upload the ag_models directory\n",
        "upload_directory_to_gcs(\n",
        "    local_path='./ag_models',\n",
        "    bucket_name='nba_award_predictor',\n",
        "    gcs_path='nba_data/models/ag_models',\n",
        "    credentials_path='cis-5450-final-project-ff442a88ac1b.json'\n",
        ")"
      ],
      "metadata": {
        "id": "TiewTcxGWARA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpH6utbX0rml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}