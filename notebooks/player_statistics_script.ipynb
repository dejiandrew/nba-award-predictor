{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b81043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing DataFrame found in memory\n",
      "Downloading files...\n",
      "\n",
      "Downloaded playerstatistics.csv\n",
      "\n",
      "Downloaded name_mappings.csv\n",
      "\n",
      "Downloaded nba_player_lookup.csv\n",
      "Processing data in chunks of 100000 rows...\n",
      "Processed chunk 1 - Total rows: 100376\n",
      "Processed chunk 2 - Total rows: 200496\n",
      "Processed chunk 3 - Total rows: 300601\n",
      "Processed chunk 4 - Total rows: 401344\n",
      "Processed chunk 5 - Total rows: 502681\n",
      "Processed chunk 6 - Total rows: 603419\n",
      "Processed chunk 7 - Total rows: 704469\n",
      "Processed chunk 8 - Total rows: 805494\n",
      "Processed chunk 9 - Total rows: 907237\n",
      "Processed chunk 10 - Total rows: 1010221\n",
      "Processed chunk 11 - Total rows: 1113653\n",
      "Processed chunk 12 - Total rows: 1216620\n",
      "Processed chunk 13 - Total rows: 1320887\n",
      "Processed chunk 14 - Total rows: 1424425\n",
      "Processed chunk 15 - Total rows: 1526924\n",
      "Processed chunk 16 - Total rows: 1627956\n",
      "Processed chunk 17 - Total rows: 1661388\n",
      "All chunks processed. Total rows: 1661388\n",
      "Results saved to player-statistics.csv\n",
      "Uploading file to Google Cloud Storage...\n",
      "File successfully uploaded to gs://nba_award_predictor/nba_data/common-player-info.csv\n",
      "Uploaded file size: 308.10 MB\n",
      "Process complete!\n"
     ]
    }
   ],
   "source": [
    "# First, clear any existing large DataFrames from memory\n",
    "try:\n",
    "    del player_statistics_df\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"Memory cleared from previous DataFrame\")\n",
    "except NameError:\n",
    "    print(\"No existing DataFrame found in memory\")\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import duckdb\n",
    "import wget\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def remove_accents(text):\n",
    "    \"\"\"\n",
    "    Remove accent marks from input text while preserving the base characters.\n",
    "    Also handles special characters like Đ/đ.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    special_chars = {\n",
    "        'Đ': 'D', 'đ': 'd', 'Ł': 'L', 'ł': 'l', 'Ø': 'O', 'ø': 'o',\n",
    "        'Ŧ': 'T', 'ŧ': 't', 'Æ': 'AE', 'æ': 'ae', 'Œ': 'OE', 'œ': 'oe',\n",
    "        'ß': 'ss'\n",
    "    }\n",
    "    \n",
    "    for char, replacement in special_chars.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    \n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    result = ''.join(c for c in normalized_text if not unicodedata.category(c).startswith('Mn'))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Download files\n",
    "print(\"Downloading files...\")\n",
    "filename = 'playerstatistics.csv'\n",
    "url = f'https://storage.googleapis.com/nba_award_predictor/nba_data/{filename}'\n",
    "wget.download(url)\n",
    "print(f\"\\nDownloaded {filename}\")\n",
    "\n",
    "filename = 'name_mappings.csv'\n",
    "url = f'https://storage.googleapis.com/nba_award_predictor/nba_data/{filename}'\n",
    "wget.download(url)\n",
    "print(f\"\\nDownloaded {filename}\")\n",
    "\n",
    "filename = 'nba_player_lookup.csv'\n",
    "url = f'https://storage.googleapis.com/nba_award_predictor/nba_data/{filename}'\n",
    "wget.download(url)\n",
    "print(f\"\\nDownloaded {filename}\")\n",
    "\n",
    "# Read in the smaller datasets fully\n",
    "name_mapping_df = pd.read_csv('name_mappings.csv')\n",
    "nba_player_lookup_df = pd.read_csv('nba_player_lookup.csv')\n",
    "\n",
    "# Clean player names in lookup table\n",
    "nba_player_lookup_df[\"player_name\"] = nba_player_lookup_df[\"player_name\"].apply(remove_accents)\n",
    "\n",
    "# Register these dataframes with DuckDB\n",
    "duckdb.register('name_mapping_df', name_mapping_df)\n",
    "duckdb.register('nba_player_lookup_df', nba_player_lookup_df)\n",
    "\n",
    "# Define the output file\n",
    "output_file = 'player-statistics.csv'\n",
    "\n",
    "# Process and save in chunks\n",
    "chunk_size = 100000\n",
    "first_chunk = True\n",
    "processed_rows = 0\n",
    "\n",
    "print(f\"Processing data in chunks of {chunk_size} rows...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(pd.read_csv('playerstatistics.csv', chunksize=chunk_size, low_memory=False)):\n",
    "    # Add full_name column\n",
    "    chunk['full_name'] = chunk['firstName'] + ' ' + chunk['lastName']\n",
    "    \n",
    "    # Register current chunk with DuckDB\n",
    "    duckdb.register('player_statistics_chunk', chunk)\n",
    "    \n",
    "    # Modified SQL query to explicitly select columns and rename player_id to player_id_1 temporarily\n",
    "    # to avoid confusion in the column selection later\n",
    "    query = \"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT * FROM player_statistics_chunk\n",
    "        LEFT JOIN name_mapping_df\n",
    "        ON player_statistics_chunk.full_name = name_mapping_df.in_table_name\n",
    "    )\n",
    "    ,CTE2 AS (\n",
    "        SELECT *,\n",
    "        CASE WHEN nba_lookup_name IS NULL THEN full_name\n",
    "        ELSE nba_lookup_name\n",
    "        END AS player_full_name\n",
    "        FROM CTE\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        CTE2.*,\n",
    "        nba_player_lookup_df.player_id AS player_id_1\n",
    "    FROM CTE2\n",
    "    LEFT JOIN nba_player_lookup_df\n",
    "    ON CTE2.player_full_name = nba_player_lookup_df.player_name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query for this chunk\n",
    "    result_chunk = duckdb.query(query).df()\n",
    "    \n",
    "    # Process the result chunk to omit unwanted columns and reorder\n",
    "    columns_to_drop = ['full_name', 'in_table_name', 'nba_lookup_name', 'player_id',\n",
    "                      'Unnamed: 3', 'player_full_name']\n",
    "    \n",
    "    # Drop unwanted columns (skip any that might not exist)\n",
    "    for col in columns_to_drop:\n",
    "        if col in result_chunk.columns:\n",
    "            result_chunk = result_chunk.drop(columns=[col])\n",
    "    \n",
    "    # Rename player_id_1 to player_id\n",
    "    if 'player_id_1' in result_chunk.columns:\n",
    "        result_chunk = result_chunk.rename(columns={'player_id_1': 'player_id'})\n",
    "    \n",
    "    # Reorder columns to put player_id first\n",
    "    if 'player_id' in result_chunk.columns:\n",
    "        cols = result_chunk.columns.tolist()\n",
    "        cols.remove('player_id')\n",
    "        result_chunk = result_chunk[['player_id'] + cols]\n",
    "    \n",
    "    # Write to CSV (first chunk with header, subsequent chunks without)\n",
    "    if first_chunk:\n",
    "        result_chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        result_chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    \n",
    "    # Update progress\n",
    "    processed_rows += len(result_chunk)\n",
    "    print(f\"Processed chunk {chunk_num+1} - Total rows: {processed_rows}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    duckdb.unregister('player_statistics_chunk')\n",
    "    del chunk\n",
    "    del result_chunk\n",
    "\n",
    "print(f\"All chunks processed. Total rows: {processed_rows}\")\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Upload to GCS\n",
    "print(\"Uploading file to Google Cloud Storage...\")\n",
    "\n",
    "# Path to your credentials file\n",
    "credentials_path = 'cis-5450-final-project-485661e2f371.json'\n",
    "\n",
    "try:\n",
    "    # Set up the client with your credentials\n",
    "    storage_client = storage.Client.from_service_account_json(credentials_path)\n",
    "    \n",
    "    # Specify your bucket name\n",
    "    bucket_name = 'nba_award_predictor'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Define blob (file in GCS) and upload from the local file\n",
    "    blob = bucket.blob('nba_data/common-player-info.csv')\n",
    "    blob.cache_control = \"max-age=0\"\n",
    "    blob.upload_from_filename(output_file)\n",
    "    \n",
    "    print(f\"File successfully uploaded to gs://{bucket_name}/nba_data/common-player-info.csv\")\n",
    "    \n",
    "    # Get file size for confirmation\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"Uploaded file size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to GCS: {e}\")\n",
    "    print(\"You may need to update the credentials file.\")\n",
    "\n",
    "print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
