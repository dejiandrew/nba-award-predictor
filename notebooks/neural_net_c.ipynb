{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cfd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c02b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['player_id', 'full_name', 'team', 'season', 'week', 'week_start',\n",
       "       'conference', 'pow_conference', 'games_played_this_week', 'numMinutes',\n",
       "       'points', 'assists', 'blocks', 'steals', 'reboundsTotal',\n",
       "       'reboundsDefensive', 'reboundsOffensive', 'fieldGoalsAttempted',\n",
       "       'fieldGoalsMade', 'threePointersAttempted', 'threePointersMade',\n",
       "       'freeThrowsAttempted', 'freeThrowsMade', 'turnovers', 'foulsPersonal',\n",
       "       'plusMinusPoints', 'wins_this_week', 'wins_vs_team_with_all_nba_player',\n",
       "       'is_win_vs_over_500', 'opponent_has_all_nba', 'avg_opp_score',\n",
       "       'avg_opp_winrate_prior', 'avg_opp_wins_prior', 'avg_opp_losses_prior',\n",
       "       'away_games_prior', 'away_losses_prior', 'away_win_streak_prior',\n",
       "       'away_wins_prior', 'home_games_prior', 'home_losses_prior',\n",
       "       'home_win_streak_prior', 'home_wins_prior', 'losses_prior',\n",
       "       'wins_vs_over_500_prior', 'won_player_of_the_week',\n",
       "       'all_star_this_season', 'mvp_this_season',\n",
       "       'all_nba_first_team_this_season', 'all_nba_second_team_this_season',\n",
       "       'all_nba_third_team_this_season', 'team_pts', 'team_ast', 'team_blk',\n",
       "       'team_stl', 'team_gms', 'pow_player_id', 'player_of_the_week',\n",
       "       'fieldGoalsPercentage', 'threePointersPercentage',\n",
       "       'freeThrowsPercentage', 'points_mean_season', 'points_std_season',\n",
       "       'assists_mean_season', 'assists_std_season',\n",
       "       'plusMinusPoints_mean_season', 'plusMinusPoints_std_season', 'z_pts',\n",
       "       'z_ast', 'z_pm', 'breakout_score', 'league_pts_mean', 'league_pts_std',\n",
       "       'league_ast_mean', 'league_ast_std', 'league_pm_mean', 'league_pm_std',\n",
       "       'z_s_pts', 'z_s_ast', 'z_s_pm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.read_csv(\"output.csv\")\n",
    "cat_cols = ['team', 'conference']\n",
    "total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bec1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To conduct week by week inference, we will train our model pre-2025-26 season and inference on our 2025-26 weeks\n",
    "total_pre = total[total['season'] != 2025]\n",
    "total_inf = total[total['season'] == 2025] # our inference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15134f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where season is before 2001-02 season\n",
    "total_pre = total_pre[total_pre['season'] > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb533ba8",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6b154",
   "metadata": {},
   "source": [
    "While we achieved 100% accuracy for our Light GBM inferences, our model will eventually reach its baseline as mentioned above. Thus, we will explore another model to see if it can achieve a higher baseline accuracy: neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e7009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Apple Silicon GPU (MPS)\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Assign device to gpu if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Training on CPU\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c730d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee843758",
   "metadata": {},
   "source": [
    "As extensively shown in our Light GBM model, we need to keep our groups (season, week, conference) in tact when training our model. As such, we need to create a custom batch sampler that keeps entire groups together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58e2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupBatchSampler:\n",
    "    def __init__(self, group_ids, batch_size):\n",
    "        self.group_ids = group_ids\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Group indices by group_id\n",
    "        self.groups = {}\n",
    "        for idx, group_id in enumerate(group_ids):\n",
    "            if group_id not in self.groups:\n",
    "                self.groups[group_id] = []\n",
    "            self.groups[group_id].append(idx)\n",
    "\n",
    "        self.group_list = list(self.groups.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle the order of the groups\n",
    "        np.random.shuffle(self.group_list)\n",
    "\n",
    "        batch = []\n",
    "        for group_id in self.group_list:\n",
    "            group_indices = self.groups[group_id]\n",
    "            batch.extend(group_indices)\n",
    "\n",
    "            # Yield batch when reching batch_size\n",
    "            while len(batch) >= self.batch_size:\n",
    "                yield batch[:self.batch_size]\n",
    "                batch = batch[self.batch_size:]\n",
    "\n",
    "         # Yield remaining samples   \n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.group_ids) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d052b5a",
   "metadata": {},
   "source": [
    "Unlike our Light GBM model, we need to one-hot encode our categorical features (team, conference) for our neural net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff0289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before one-hot encoding: 65\n",
      "Number of features after one-hot encoding: 95\n"
     ]
    }
   ],
   "source": [
    "# From our data above\n",
    "# Reminder: total_pre = data up to 2024-25 season\n",
    "X = total_pre.drop(columns=['full_name', 'player_id', 'pow_player_id', 'player_of_the_week', 'won_player_of_the_week', 'all_star_this_season', 'mvp_this_season',\n",
    " 'all_nba_first_team_this_season', 'all_nba_second_team_this_season', 'all_nba_third_team_this_season', 'week_start', 'pow_conference',\n",
    " 'is_win_vs_over_500', 'opponent_has_all_nba'])\n",
    "\n",
    "y = total_pre['won_player_of_the_week']\n",
    "\n",
    "print(f\"Number of features before one-hot encoding: {len(X.columns)}\")\n",
    "\n",
    "# One-hot encode\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Rename conference one-hot encoding column: 'conference_West' -> 'conference'\n",
    "X = X.rename(columns={'conference_West': 'conference'})\n",
    "\n",
    "print(f\"Number of features after one-hot encoding: {len(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589df5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'week', 'games_played_this_week', 'numMinutes', 'points',\n",
       "       'assists', 'blocks', 'steals', 'reboundsTotal', 'reboundsDefensive',\n",
       "       'reboundsOffensive', 'fieldGoalsAttempted', 'fieldGoalsMade',\n",
       "       'threePointersAttempted', 'threePointersMade', 'freeThrowsAttempted',\n",
       "       'freeThrowsMade', 'turnovers', 'foulsPersonal', 'plusMinusPoints',\n",
       "       'wins_this_week', 'wins_vs_team_with_all_nba_player', 'avg_opp_score',\n",
       "       'avg_opp_winrate_prior', 'avg_opp_wins_prior', 'avg_opp_losses_prior',\n",
       "       'away_games_prior', 'away_losses_prior', 'away_win_streak_prior',\n",
       "       'away_wins_prior', 'home_games_prior', 'home_losses_prior',\n",
       "       'home_win_streak_prior', 'home_wins_prior', 'losses_prior',\n",
       "       'wins_vs_over_500_prior', 'team_pts', 'team_ast', 'team_blk',\n",
       "       'team_stl', 'team_gms', 'fieldGoalsPercentage',\n",
       "       'threePointersPercentage', 'freeThrowsPercentage', 'points_mean_season',\n",
       "       'points_std_season', 'assists_mean_season', 'assists_std_season',\n",
       "       'plusMinusPoints_mean_season', 'plusMinusPoints_std_season', 'z_pts',\n",
       "       'z_ast', 'z_pm', 'breakout_score', 'league_pts_mean', 'league_pts_std',\n",
       "       'league_ast_mean', 'league_ast_std', 'league_pm_mean', 'league_pm_std',\n",
       "       'z_s_pts', 'z_s_ast', 'z_s_pm', 'team_Bobcats', 'team_Bucks',\n",
       "       'team_Bulls', 'team_Cavaliers', 'team_Celtics', 'team_Clippers',\n",
       "       'team_Grizzlies', 'team_Hawks', 'team_Heat', 'team_Hornets',\n",
       "       'team_Jazz', 'team_Kings', 'team_Knicks', 'team_Lakers', 'team_Magic',\n",
       "       'team_Mavericks', 'team_Nets', 'team_Nuggets', 'team_Pacers',\n",
       "       'team_Pelicans', 'team_Pistons', 'team_Raptors', 'team_Rockets',\n",
       "       'team_Spurs', 'team_Suns', 'team_SuperSonics', 'team_Thunder',\n",
       "       'team_Timberwolves', 'team_Trail Blazers', 'team_Warriors',\n",
       "       'team_Wizards', 'conference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "925c4585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ec5cb",
   "metadata": {},
   "source": [
    "Since we one-hot encoded our categorical features, the number of features will grow a lot due to the number of teams in the NBA. This is totally ok since neural networks can handle high dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e47d4a",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6119523",
   "metadata": {},
   "source": [
    "Our neural network has three layers that get progressively smaller (256 -> 128 -> 64 neurons), like a funnel that squeezes information down from ~100 input features into a single score per player. The first layer spots basic patterns in the data, the middle layer combines these patterns into bigger ideas like \"great scorer\" or \"well-rounded player,\" and the final layer picks out what actually matters for winning player of the week. \n",
    "\n",
    "Between each layer, we use ReLU activation which lets the network learn that certain combinations of stats matter more together than separately (like how 35 points + 3 wins is way more impressive than just 35 points alone). We also randomly turn off 30% of neurons during training (dropout) so the network doesn't just memorize the data and actually learns real patterns. The final output is a ranking score rather than a probability because we just need to know who scores highest, not the exact chance they win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e524fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POTWRanker(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super(POTWRanker, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (single score per player)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352112c3",
   "metadata": {},
   "source": [
    "### Ranking Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949a1d9",
   "metadata": {},
   "source": [
    "Our loss function teaches the model to make sure winners score higher than everyone else in the same week, rather than just predicting winners independently. For every week, it compares the winner to every non-winner (so if there's 1 winner and 50 other players, it makes 50 comparisons) and gives the model a penalty whenever the winner's score isn't at least 1.0 points higher. This is like telling the model \"the winner needs to clearly beat everyone else, not just barely edge them out.\" The key advantage is that the model learns what makes one player better than another in that specific week's context, rather than trying to learn some universal \"good player\" score. This only works if we keep entire weeks together during training. If we mix players from different weeks, the comparisons don't make sense since those players never actually competed for the same award."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d262f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(PairwiseRankingLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, scores, labels, group_indices):\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        num_pairs = 0\n",
    "        \n",
    "        # For each group (week/conference)\n",
    "        for group_id in torch.unique(group_indices):\n",
    "            mask = group_indices == group_id\n",
    "            group_scores = scores[mask].squeeze()\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            # Find winners and non-winners\n",
    "            winner_mask = group_labels == 1\n",
    "            non_winner_mask = group_labels == 0\n",
    "            \n",
    "            if winner_mask.sum() == 0 or non_winner_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            winner_scores = group_scores[winner_mask]\n",
    "            non_winner_scores = group_scores[non_winner_mask]\n",
    "            \n",
    "            # Compute all pairwise differences at once\n",
    "            score_diff = winner_scores.unsqueeze(1) - non_winner_scores.unsqueeze(0)\n",
    "            \n",
    "            # Apply margin and clamp\n",
    "            losses = torch.clamp(self.margin - score_diff, min=0)\n",
    "            \n",
    "            total_loss = total_loss + losses.sum()  # Add to tensor\n",
    "            num_pairs += losses.numel()\n",
    "        \n",
    "        # Return tensor (even if zero)\n",
    "        if num_pairs == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        return total_loss / num_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ca875",
   "metadata": {},
   "source": [
    "### Cross-validation Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8c74e",
   "metadata": {},
   "source": [
    "Again, we will run time-based cross validation to prevent overfitting results swaying our decisions. We will follow the same format as our cross validation from Light GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60d231",
   "metadata": {},
   "source": [
    "Unlike our Light GBM model, in our neural net model, we will scale our data because neural networks are sensitive to feature magnitudesâ€”features. For example, points would dominate features like fieldGoalsPercentage during training, causing the model to learn poorly. Standardization ensures all features contribute equally to the model's learning process and helps gradient descent converge faster and more reliably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e82c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 1\n",
      "Val Season: 2024\n",
      "Train Seasons: 2001-2023\n",
      "Train size: 165504, Val size: 7971\n",
      "  Number of features: 95\n",
      "Train shape: (165504, 95)\n",
      "Val shape: (7971, 95)\n",
      "\n",
      "Creating PyTorch dataset...\n",
      "\n",
      "Creating GroupBatchSampler and DataLoader...\n",
      "DataLoader created. Number of batches: 324\n",
      "\n",
      "Initializing model...\n",
      "Model created. Parameters: 221,697\n",
      "Model moved to mps\n",
      "\n",
      "Setting up loss and optimizer...\n",
      "Loss and optimizer ready\n",
      "\n",
      "Training 50 epochs\n",
      "Epoch 10/50, Loss: 0.0087\n",
      "Epoch 20/50, Loss: 0.0050\n",
      "Epoch 30/50, Loss: 0.0025\n",
      "Epoch 40/50, Loss: 0.0021\n",
      "Epoch 50/50, Loss: 0.0018\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Fold 1 Results:\n",
      "Top-1 Accuracy: 0.4000\n",
      "Top-3 Accuracy: 0.6250\n",
      "Top-5 Accuracy: 0.7250\n",
      "Top Rank: 1\n",
      "Lowest Rank: 22\n",
      "Validation 2\n",
      "Val Season: 2023\n",
      "Train Seasons: 2001-2022\n",
      "Train size: 158395, Val size: 7109\n",
      "  Number of features: 95\n",
      "Train shape: (158395, 95)\n",
      "Val shape: (7109, 95)\n",
      "\n",
      "Creating PyTorch dataset...\n",
      "\n",
      "Creating GroupBatchSampler and DataLoader...\n",
      "DataLoader created. Number of batches: 310\n",
      "\n",
      "Initializing model...\n",
      "Model created. Parameters: 221,697\n",
      "Model moved to mps\n",
      "\n",
      "Setting up loss and optimizer...\n",
      "Loss and optimizer ready\n",
      "\n",
      "Training 50 epochs\n",
      "Epoch 10/50, Loss: 0.0086\n",
      "Epoch 20/50, Loss: 0.0046\n",
      "Epoch 30/50, Loss: 0.0048\n",
      "Epoch 40/50, Loss: 0.0012\n",
      "Epoch 50/50, Loss: 0.0013\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Fold 2 Results:\n",
      "Top-1 Accuracy: 0.4857\n",
      "Top-3 Accuracy: 0.7714\n",
      "Top-5 Accuracy: 0.9143\n",
      "Top Rank: 1\n",
      "Lowest Rank: 18\n",
      "Validation 3\n",
      "Val Season: 2022\n",
      "Train Seasons: 2001-2021\n",
      "Train size: 150277, Val size: 8118\n",
      "  Number of features: 95\n",
      "Train shape: (150277, 95)\n",
      "Val shape: (8118, 95)\n",
      "\n",
      "Creating PyTorch dataset...\n",
      "\n",
      "Creating GroupBatchSampler and DataLoader...\n",
      "DataLoader created. Number of batches: 294\n",
      "\n",
      "Initializing model...\n",
      "Model created. Parameters: 221,697\n",
      "Model moved to mps\n",
      "\n",
      "Setting up loss and optimizer...\n",
      "Loss and optimizer ready\n",
      "\n",
      "Training 50 epochs\n",
      "Epoch 10/50, Loss: 0.0087\n",
      "Epoch 20/50, Loss: 0.0048\n",
      "Epoch 30/50, Loss: 0.0032\n",
      "Epoch 40/50, Loss: 0.0022\n",
      "Epoch 50/50, Loss: 0.0011\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Fold 3 Results:\n",
      "Top-1 Accuracy: 0.5952\n",
      "Top-3 Accuracy: 0.8571\n",
      "Top-5 Accuracy: 0.8810\n",
      "Top Rank: 1\n",
      "Lowest Rank: 35\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "hidden_dims = [512, 256, 128, 64]\n",
    "dropout = 0.2\n",
    "lr = 0.001\n",
    "\n",
    "unique_seasons = sorted(X['season'].unique())\n",
    "folds = 3  # Reduced from 5 to 3 compared to Light GBM\n",
    "k = [1, 3, 5, 10]\n",
    "cv_results = []\n",
    "\n",
    "for fold in range(1, folds + 1):\n",
    "    val_season = unique_seasons[-fold]\n",
    "    train_seasons = unique_seasons[:-fold]\n",
    "    \n",
    "    val_mask = X['season'] == val_season\n",
    "    train_mask = X['season'].isin(train_seasons)\n",
    "    \n",
    "    X_train = X[train_mask].copy()\n",
    "    y_train = y[train_mask].copy()\n",
    "    X_val = X[val_mask].copy()\n",
    "    y_val = y[val_mask].copy()\n",
    "\n",
    "    print(f\"Validation {fold}\")\n",
    "    print(f\"Val Season: {val_season}\")\n",
    "    print(f\"Train Seasons: {train_seasons[0]}-{train_seasons[-1]}\")\n",
    "    print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "    \n",
    "    X_train['group_id'] = X_train.groupby(['season', 'week', 'conference']).ngroup()\n",
    "    X_val['group_id'] = X_val.groupby(['season', 'week', 'conference']).ngroup()\n",
    "    train_group_ids = X_train['group_id'].values\n",
    "    val_group_ids = X_val['group_id'].values\n",
    "    \n",
    "    # Remove only metadata. This includes season and week. We will have our normalized 'z_' metrics capture temporal changes\n",
    "    feature_cols = [col for col in X_train.columns \n",
    "                    if col not in ['group_id']]\n",
    "    \n",
    "    X_train_features = X_train[feature_cols].values\n",
    "    X_val_features = X_val[feature_cols].values\n",
    "    print(f\"Train shape: {X_train_features.shape}\")\n",
    "    print(f\"Val shape: {X_val_features.shape}\")\n",
    "    \n",
    "    # Scaling our data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_features).astype(np.float32)\n",
    "    X_val_scaled = scaler.transform(X_val_features).astype(np.float32)\n",
    "    \n",
    "    print(\"\\nCreating PyTorch dataset...\")\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_train_scaled),\n",
    "        torch.from_numpy(y_train.values.astype(np.float32)),\n",
    "        torch.from_numpy(train_group_ids.astype(np.int64))\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCreating GroupBatchSampler and DataLoader...\")\n",
    "    batch_sampler = GroupBatchSampler(train_group_ids, batch_size=batch_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_sampler=batch_sampler)\n",
    "    print(f\"DataLoader created. Number of batches: {len(train_loader)}\")\n",
    "    \n",
    "    print(\"\\nInitializing model...\")\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    model = POTWRanker(input_dim, hidden_dims=hidden_dims, dropout=dropout)\n",
    "    print(f\"Model created. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "    \n",
    "    print(\"\\nSetting up loss and optimizer...\")\n",
    "    criterion = PairwiseRankingLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(\"Loss and optimizer ready\")\n",
    "    \n",
    "    print(f\"\\nTraining {epochs} epochs\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y, batch_groups in train_loader:\n",
    "            # Move batch data to GPU\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_groups = batch_groups.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(batch_X)\n",
    "            loss = criterion(scores, batch_y, batch_groups)\n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update weights * learning rate (Gradient descent)\n",
    "            \n",
    "            total_loss += loss.item() # Track running total loss\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nEvaluation\")\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.from_numpy(X_val_scaled).to(device)\n",
    "        val_scores = model(X_val_tensor).cpu().squeeze().numpy()\n",
    "    \n",
    "    df_val = X_val[['season', 'week', 'conference']].copy().reset_index(drop=True)\n",
    "    df_val['score'] = val_scores\n",
    "    df_val['y_true'] = y_val.reset_index(drop=True).values\n",
    "    \n",
    "    k_dict = {i: [] for i in k}\n",
    "    groups = df_val.groupby(['season', 'week', 'conference'], observed=True)\n",
    "    ranks = []\n",
    "    \n",
    "    for _, group in groups:\n",
    "        if group['y_true'].sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        group_sorted = group.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        pos_idx = group_sorted.index[group_sorted['y_true'] == 1]\n",
    "        \n",
    "        for i in k:\n",
    "            top_k = group_sorted.head(i)\n",
    "            hit = top_k['y_true'].max()\n",
    "            k_dict[i].append(hit)\n",
    "        \n",
    "        for idx in pos_idx:\n",
    "            ranks.append(idx + 1)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "    curr = {\n",
    "        \"Fold\": fold,\n",
    "        \"Top_rank\": ranks.min(),\n",
    "        \"Lowest_rank\": ranks.max(),\n",
    "        \"Percentiles\": np.percentile(ranks, [10, 25, 50, 75, 90])\n",
    "    }\n",
    "    for i, hits in k_dict.items():\n",
    "        curr[f\"Top_{i}_avg_hits\"] = np.mean(hits)\n",
    "    cv_results.append(curr)\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"Top-1 Accuracy: {curr['Top_1_avg_hits']:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {curr['Top_3_avg_hits']:.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {curr['Top_5_avg_hits']:.4f}\")\n",
    "    print(f\"Top Rank: {curr['Top_rank']}\")\n",
    "    print(f\"Lowest Rank: {curr['Lowest_rank']}\")\n",
    "    \n",
    "    # Clean up memory after each fold to clear up RAM\n",
    "    del model, optimizer, criterion, train_loader, train_dataset, batch_sampler\n",
    "    del X_train, y_train, X_val, y_val\n",
    "    del X_train_features, X_val_features, X_train_scaled, X_val_scaled\n",
    "    del train_group_ids, val_group_ids\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db95134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Top-1=0.4000, Top-5=0.7250, Top_rank=1, Lowest_rank=22, Rank_Percentile=[ 1.   1.   3.   6.  10.8]\n",
      "Fold 2: Top-1=0.4857, Top-5=0.9143, Top_rank=1, Lowest_rank=18, Rank_Percentile=[1.  1.  2.  4.  6.2]\n",
      "Fold 3: Top-1=0.5952, Top-5=0.8810, Top_rank=1, Lowest_rank=35, Rank_Percentile=[1.  1.  1.  3.  7.6]\n",
      "\n",
      "Average Performance Across Folds:\n",
      "Top-1 Accuracy: 0.4936507936507937\n",
      "Top-3 Accuracy: 0.7511904761904762\n",
      "Top-5 Accuracy: 0.8400793650793651\n",
      "Top-10 Accuracy: 0.9412698412698411\n"
     ]
    }
   ],
   "source": [
    "# Final Results\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "for idx, row in cv_df.iterrows():\n",
    "    print(f\"Fold {row['Fold']}: Top-1={row['Top_1_avg_hits']:.4f}, \"\n",
    "          f\"Top-5={row['Top_5_avg_hits']:.4f}, Top_rank={row['Top_rank']}, \"\n",
    "          f\"Lowest_rank={row['Lowest_rank']}, Rank_Percentile={row['Percentiles']}\")\n",
    "\n",
    "print(\"\\nAverage Performance Across Folds:\")\n",
    "\n",
    "for i in k:\n",
    "    print(f\"Top-{i} Accuracy: {cv_df[f'Top_{i}_avg_hits'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49725eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMPARISON WITH LIGHTGBM\")\n",
    "print(f\"LightGBM Top-1:        53.0%\")\n",
    "print(f\"Neural Network Top-1:  {cv_df['Top_1_avg_hits'].mean():.1%}\")\n",
    "print(f\"Difference:            {(cv_df['Top_1_avg_hits'].mean() - 0.53)*100:+.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
