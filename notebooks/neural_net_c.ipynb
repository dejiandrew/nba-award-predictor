{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41cfd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshualim/nba-award-predictor/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c02b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['player_id', 'full_name', 'team', 'season', 'week', 'week_start',\n",
       "       'conference', 'pow_conference', 'games_played_this_week', 'numMinutes',\n",
       "       'points', 'assists', 'blocks', 'steals', 'reboundsTotal',\n",
       "       'reboundsDefensive', 'reboundsOffensive', 'fieldGoalsAttempted',\n",
       "       'fieldGoalsMade', 'threePointersAttempted', 'threePointersMade',\n",
       "       'freeThrowsAttempted', 'freeThrowsMade', 'turnovers', 'foulsPersonal',\n",
       "       'plusMinusPoints', 'wins_this_week', 'wins_vs_team_with_all_nba_player',\n",
       "       'is_win_vs_over_500', 'opponent_has_all_nba', 'avg_opp_score',\n",
       "       'avg_opp_winrate_prior', 'avg_opp_wins_prior', 'avg_opp_losses_prior',\n",
       "       'away_games_prior', 'away_losses_prior', 'away_win_streak_prior',\n",
       "       'away_wins_prior', 'home_games_prior', 'home_losses_prior',\n",
       "       'home_win_streak_prior', 'home_wins_prior', 'losses_prior',\n",
       "       'wins_vs_over_500_prior', 'won_player_of_the_week',\n",
       "       'all_star_this_season', 'mvp_this_season',\n",
       "       'all_nba_first_team_this_season', 'all_nba_second_team_this_season',\n",
       "       'all_nba_third_team_this_season', 'team_pts', 'team_ast', 'team_blk',\n",
       "       'team_stl', 'team_gms', 'pow_player_id', 'player_of_the_week',\n",
       "       'fieldGoalsPercentage', 'threePointersPercentage',\n",
       "       'freeThrowsPercentage', 'points_mean_season', 'points_std_season',\n",
       "       'assists_mean_season', 'assists_std_season',\n",
       "       'plusMinusPoints_mean_season', 'plusMinusPoints_std_season', 'z_pts',\n",
       "       'z_ast', 'z_pm', 'breakout_score', 'league_pts_mean', 'league_pts_std',\n",
       "       'league_ast_mean', 'league_ast_std', 'league_pm_mean', 'league_pm_std',\n",
       "       'z_s_pts', 'z_s_ast', 'z_s_pm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = pd.read_csv(\"output.csv\")\n",
    "cat_cols = ['team', 'conference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bec1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To conduct week by week inference, we will train our model pre-2025-26 season and inference on our 2025-26 weeks\n",
    "total_pre = total[total['season'] != 2025]\n",
    "total_inf = total[total['season'] == 2025] # our inference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15134f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where season is before 2001-02 season\n",
    "total_pre = total_pre[total_pre['season'] > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb533ba8",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6b154",
   "metadata": {},
   "source": [
    "While we achieved 100% accuracy for our Light GBM inferences, our model will eventually reach its baseline as mentioned above. Thus, we will explore another model to see if it can achieve a higher baseline accuracy: neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e7009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Apple Silicon GPU (MPS)\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Assign device to gpu if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Training on CPU\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c730d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee843758",
   "metadata": {},
   "source": [
    "As extensively shown in our Light GBM model, we need to keep our groups (season, week, conference) in tact when training our model. As such, we need to create a custom batch sampler that keeps entire groups together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58e2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupBatchSampler:\n",
    "    def __init__(self, group_ids, batch_size):\n",
    "        self.group_ids = group_ids\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Group indices by group_id\n",
    "        self.groups = {}\n",
    "        for idx, group_id in enumerate(group_ids):\n",
    "            if group_id not in self.groups:\n",
    "                self.groups[group_id] = []\n",
    "            self.groups[group_id].append(idx)\n",
    "\n",
    "        self.group_list = list(self.groups.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle the order of the groups\n",
    "        np.random.shuffle(self.group_list)\n",
    "\n",
    "        batch = []\n",
    "        for group_id in self.group_list:\n",
    "            group_indices = self.groups[group_id]\n",
    "            batch.extend(group_indices)\n",
    "\n",
    "            # Yield batch when reching batch_size\n",
    "            while len(batch) >= self.batch_size:\n",
    "                yield batch[:self.batch_size]\n",
    "                batch = batch[self.batch_size:]\n",
    "\n",
    "         # Yield remaining samples   \n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.group_ids) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d052b5a",
   "metadata": {},
   "source": [
    "Unlike our Light GBM model, we need to one-hot encode our categorical features (team, conference) for our neural net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff0289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before one-hot encoding: 65\n",
      "Number of features after one-hot encoding: 95\n"
     ]
    }
   ],
   "source": [
    "# From our data above\n",
    "# Reminder: total_pre = data up to 2024-25 season\n",
    "X = total_pre.drop(columns=['full_name', 'player_id', 'pow_player_id', 'player_of_the_week', 'won_player_of_the_week', 'all_star_this_season', 'mvp_this_season',\n",
    " 'all_nba_first_team_this_season', 'all_nba_second_team_this_season', 'all_nba_third_team_this_season', 'week_start', 'pow_conference',\n",
    " 'is_win_vs_over_500', 'opponent_has_all_nba'])\n",
    "\n",
    "y = total_pre['won_player_of_the_week']\n",
    "\n",
    "print(f\"Number of features before one-hot encoding: {len(X.columns)}\")\n",
    "\n",
    "# One-hot encode\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Rename conference one-hot encoding column: 'conference_West' -> 'conference'\n",
    "X = X.rename(columns={'conference_West': 'conference'})\n",
    "\n",
    "print(f\"Number of features after one-hot encoding: {len(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589df5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'week', 'games_played_this_week', 'numMinutes', 'points',\n",
       "       'assists', 'blocks', 'steals', 'reboundsTotal', 'reboundsDefensive',\n",
       "       'reboundsOffensive', 'fieldGoalsAttempted', 'fieldGoalsMade',\n",
       "       'threePointersAttempted', 'threePointersMade', 'freeThrowsAttempted',\n",
       "       'freeThrowsMade', 'turnovers', 'foulsPersonal', 'plusMinusPoints',\n",
       "       'wins_this_week', 'wins_vs_team_with_all_nba_player', 'avg_opp_score',\n",
       "       'avg_opp_winrate_prior', 'avg_opp_wins_prior', 'avg_opp_losses_prior',\n",
       "       'away_games_prior', 'away_losses_prior', 'away_win_streak_prior',\n",
       "       'away_wins_prior', 'home_games_prior', 'home_losses_prior',\n",
       "       'home_win_streak_prior', 'home_wins_prior', 'losses_prior',\n",
       "       'wins_vs_over_500_prior', 'team_pts', 'team_ast', 'team_blk',\n",
       "       'team_stl', 'team_gms', 'fieldGoalsPercentage',\n",
       "       'threePointersPercentage', 'freeThrowsPercentage', 'points_mean_season',\n",
       "       'points_std_season', 'assists_mean_season', 'assists_std_season',\n",
       "       'plusMinusPoints_mean_season', 'plusMinusPoints_std_season', 'z_pts',\n",
       "       'z_ast', 'z_pm', 'breakout_score', 'league_pts_mean', 'league_pts_std',\n",
       "       'league_ast_mean', 'league_ast_std', 'league_pm_mean', 'league_pm_std',\n",
       "       'z_s_pts', 'z_s_ast', 'z_s_pm', 'team_Bobcats', 'team_Bucks',\n",
       "       'team_Bulls', 'team_Cavaliers', 'team_Celtics', 'team_Clippers',\n",
       "       'team_Grizzlies', 'team_Hawks', 'team_Heat', 'team_Hornets',\n",
       "       'team_Jazz', 'team_Kings', 'team_Knicks', 'team_Lakers', 'team_Magic',\n",
       "       'team_Mavericks', 'team_Nets', 'team_Nuggets', 'team_Pacers',\n",
       "       'team_Pelicans', 'team_Pistons', 'team_Raptors', 'team_Rockets',\n",
       "       'team_Spurs', 'team_Suns', 'team_SuperSonics', 'team_Thunder',\n",
       "       'team_Timberwolves', 'team_Trail Blazers', 'team_Warriors',\n",
       "       'team_Wizards', 'conference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ec5cb",
   "metadata": {},
   "source": [
    "Since we one-hot encoded our categorical features, the number of features will grow a lot due to the number of teams in the NBA. This is totally ok since neural networks can handle high dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e47d4a",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6119523",
   "metadata": {},
   "source": [
    "Our neural network has three layers that get progressively smaller (256 -> 128 -> 64 neurons), like a funnel that squeezes information down from ~100 input features into a single score per player. The first layer spots basic patterns in the data, the middle layer combines these patterns into bigger ideas like \"great scorer\" or \"well-rounded player,\" and the final layer picks out what actually matters for winning player of the week. \n",
    "\n",
    "Between each layer, we use ReLU activation which lets the network learn that certain combinations of stats matter more together than separately (like how 35 points + 3 wins is way more impressive than just 35 points alone). We also randomly turn off 30% of neurons during training (dropout) so the network doesn't just memorize the data and actually learns real patterns. The final output is a ranking score rather than a probability because we just need to know who scores highest, not the exact chance they win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e524fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POTWRanker(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super(POTWRanker, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (single score per player)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352112c3",
   "metadata": {},
   "source": [
    "### Ranking Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949a1d9",
   "metadata": {},
   "source": [
    "Our loss function teaches the model to make sure winners score higher than everyone else in the same week, rather than just predicting winners independently. For every week, it compares the winner to every non-winner (so if there's 1 winner and 50 other players, it makes 50 comparisons) and gives the model a penalty whenever the winner's score isn't at least 1.0 points higher. This is like telling the model \"the winner needs to clearly beat everyone else, not just barely edge them out.\" The key advantage is that the model learns what makes one player better than another in that specific week's context, rather than trying to learn some universal \"good player\" score. This only works if we keep entire weeks together during training. If we mix players from different weeks, the comparisons don't make sense since those players never actually competed for the same award."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d262f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(PairwiseRankingLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, scores, labels, group_indices):\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        num_pairs = 0\n",
    "        \n",
    "        # For each group (week/conference)\n",
    "        for group_id in torch.unique(group_indices):\n",
    "            mask = group_indices == group_id\n",
    "            group_scores = scores[mask].squeeze()\n",
    "            group_labels = labels[mask]\n",
    "            \n",
    "            # Find winners and non-winners\n",
    "            winner_mask = group_labels == 1\n",
    "            non_winner_mask = group_labels == 0\n",
    "            \n",
    "            if winner_mask.sum() == 0 or non_winner_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            winner_scores = group_scores[winner_mask]\n",
    "            non_winner_scores = group_scores[non_winner_mask]\n",
    "            \n",
    "            # Compute all pairwise differences at once\n",
    "            score_diff = winner_scores.unsqueeze(1) - non_winner_scores.unsqueeze(0)\n",
    "            \n",
    "            # Apply margin and clamp\n",
    "            losses = torch.clamp(self.margin - score_diff, min=0)\n",
    "            \n",
    "            total_loss = total_loss + losses.sum()  # Add to tensor\n",
    "            num_pairs += losses.numel()\n",
    "        \n",
    "        # Return tensor (even if zero)\n",
    "        if num_pairs == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        return total_loss / num_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ca875",
   "metadata": {},
   "source": [
    "### Cross-validation Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8c74e",
   "metadata": {},
   "source": [
    "Again, we will run time-based cross validation to prevent overfitting results swaying our decisions. We will follow the same format as our cross validation from Light GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60d231",
   "metadata": {},
   "source": [
    "Unlike our Light GBM model, in our neural net model, we will scale our data because neural networks are sensitive to feature magnitudes. For example, 'points' (0 to 70+ range) would dominate features like 'fieldGoalsPercentage' (0 to 1 range) during training, causing the model to learn poorly. Standardization ensures all features contribute equally to the model's learning process and helps gradient descent converge faster and more reliably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 1\n",
      "Val Season: 2024\n",
      "Train Seasons: 2001-2023\n",
      "Train size: 165504, Val size: 7971\n",
      "  Number of features: 95\n",
      "Train shape: (165504, 95)\n",
      "Val shape: (7971, 95)\n",
      "\n",
      "Creating PyTorch dataset...\n",
      "\n",
      "Creating GroupBatchSampler and DataLoader...\n",
      "DataLoader created. Number of batches: 324\n",
      "\n",
      "Initializing model...\n",
      "Model created. Parameters: 221,697\n",
      "Model moved to mps\n",
      "\n",
      "Setting up loss and optimizer...\n",
      "Loss and optimizer ready\n",
      "\n",
      "Training 50 epochs\n",
      "Epoch 10/50, Loss: 0.0087\n",
      "Epoch 20/50, Loss: 0.0050\n",
      "Epoch 30/50, Loss: 0.0025\n",
      "Epoch 40/50, Loss: 0.0021\n",
      "Epoch 50/50, Loss: 0.0018\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Fold 1 Results:\n",
      "Top-1 Accuracy: 0.4000\n",
      "Top-3 Accuracy: 0.6250\n",
      "Top-5 Accuracy: 0.7250\n",
      "Top Rank: 1\n",
      "Lowest Rank: 22\n",
      "Validation 2\n",
      "Val Season: 2023\n",
      "Train Seasons: 2001-2022\n",
      "Train size: 158395, Val size: 7109\n",
      "  Number of features: 95\n",
      "Train shape: (158395, 95)\n",
      "Val shape: (7109, 95)\n",
      "\n",
      "Creating PyTorch dataset...\n",
      "\n",
      "Creating GroupBatchSampler and DataLoader...\n",
      "DataLoader created. Number of batches: 310\n",
      "\n",
      "Initializing model...\n",
      "Model created. Parameters: 221,697\n",
      "Model moved to mps\n",
      "\n",
      "Setting up loss and optimizer...\n",
      "Loss and optimizer ready\n",
      "\n",
      "Training 50 epochs\n",
      "Epoch 10/50, Loss: 0.0086\n",
      "Epoch 20/50, Loss: 0.0046\n",
      "Epoch 30/50, Loss: 0.0048\n",
      "Epoch 40/50, Loss: 0.0012\n",
      "Epoch 50/50, Loss: 0.0013\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Fold 2 Results:\n",
      "Top-1 Accuracy: 0.4857\n",
      "Top-3 Accuracy: 0.7714\n",
      "Top-5 Accuracy: 0.9143\n",
      "Top Rank: 1\n",
      "Lowest Rank: 18\n",
      "Validation 3\n",
      "Val Season: 2022\n",
      "Train Seasons: 2001-2021\n",
      "Train size: 150277, Val size: 8118\n",
      "  Number of features: 95\n",
      "Train shape: (150277, 95)\n",
      "Val shape: (8118, 95)\n",
      "\n",
      "Creating PyTorch dataset...\n",
      "\n",
      "Creating GroupBatchSampler and DataLoader...\n",
      "DataLoader created. Number of batches: 294\n",
      "\n",
      "Initializing model...\n",
      "Model created. Parameters: 221,697\n",
      "Model moved to mps\n",
      "\n",
      "Setting up loss and optimizer...\n",
      "Loss and optimizer ready\n",
      "\n",
      "Training 50 epochs\n",
      "Epoch 10/50, Loss: 0.0087\n",
      "Epoch 20/50, Loss: 0.0048\n",
      "Epoch 30/50, Loss: 0.0032\n",
      "Epoch 40/50, Loss: 0.0022\n",
      "Epoch 50/50, Loss: 0.0011\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Fold 3 Results:\n",
      "Top-1 Accuracy: 0.5952\n",
      "Top-3 Accuracy: 0.8571\n",
      "Top-5 Accuracy: 0.8810\n",
      "Top Rank: 1\n",
      "Lowest Rank: 35\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "hidden_dims = [512, 256, 128, 64]\n",
    "dropout = 0.2\n",
    "lr = 0.001\n",
    "\n",
    "unique_seasons = sorted(X['season'].unique())\n",
    "folds = 3  # Reduced from 5 to 3 compared to Light GBM\n",
    "k = [1, 3, 5, 10]\n",
    "cv_results = []\n",
    "\n",
    "for fold in range(1, folds + 1):\n",
    "    val_season = unique_seasons[-fold]\n",
    "    train_seasons = unique_seasons[:-fold]\n",
    "    \n",
    "    val_mask = X['season'] == val_season\n",
    "    train_mask = X['season'].isin(train_seasons)\n",
    "    \n",
    "    X_train = X[train_mask].copy()\n",
    "    y_train = y[train_mask].copy()\n",
    "    X_val = X[val_mask].copy()\n",
    "    y_val = y[val_mask].copy()\n",
    "\n",
    "    print(f\"Validation {fold}\")\n",
    "    print(f\"Val Season: {val_season}\")\n",
    "    print(f\"Train Seasons: {train_seasons[0]}-{train_seasons[-1]}\")\n",
    "    print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "    \n",
    "    X_train['group_id'] = X_train.groupby(['season', 'week', 'conference']).ngroup()\n",
    "    train_group_ids = X_train['group_id'].values\n",
    "    \n",
    "    X_train_features = X_train.drop(columns=['group_id']).values\n",
    "    X_val_features = X_val.values\n",
    "    print(f\"Train shape: {X_train_features.shape}\")\n",
    "    print(f\"Val shape: {X_val_features.shape}\")\n",
    "    \n",
    "    # Scaling our data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_features).astype(np.float32)\n",
    "    X_val_scaled = scaler.transform(X_val_features).astype(np.float32)\n",
    "    \n",
    "    print(\"\\nCreating PyTorch dataset...\")\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_train_scaled),\n",
    "        torch.from_numpy(y_train.values.astype(np.float32)),\n",
    "        torch.from_numpy(train_group_ids.astype(np.int64))\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCreating GroupBatchSampler and DataLoader...\")\n",
    "    batch_sampler = GroupBatchSampler(train_group_ids, batch_size=batch_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_sampler=batch_sampler)\n",
    "    print(f\"DataLoader created. Number of batches: {len(train_loader)}\")\n",
    "    \n",
    "    print(\"\\nInitializing model...\")\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    model = POTWRanker(input_dim, hidden_dims=hidden_dims, dropout=dropout)\n",
    "    print(f\"Model created. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "    \n",
    "    print(\"\\nSetting up loss and optimizer...\")\n",
    "    criterion = PairwiseRankingLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(\"Loss and optimizer ready\")\n",
    "    \n",
    "    print(f\"\\nTraining {epochs} epochs\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y, batch_groups in train_loader:\n",
    "            # Move batch data to GPU\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_groups = batch_groups.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(batch_X)\n",
    "            loss = criterion(scores, batch_y, batch_groups)\n",
    "            loss.backward() # Compute gradients\n",
    "            optimizer.step() # Update weights * learning rate (Gradient descent)\n",
    "            \n",
    "            total_loss += loss.item() # Track running total loss\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nEvaluation\")\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.from_numpy(X_val_scaled).to(device)\n",
    "        val_scores = model(X_val_tensor).cpu().squeeze().numpy()\n",
    "    \n",
    "    df_val = X_val[['season', 'week', 'conference']].copy().reset_index(drop=True)\n",
    "    df_val['score'] = val_scores\n",
    "    df_val['y_true'] = y_val.reset_index(drop=True).values\n",
    "    \n",
    "    k_dict = {i: [] for i in k}\n",
    "    groups = df_val.groupby(['season', 'week', 'conference'], observed=True)\n",
    "    ranks = []\n",
    "    \n",
    "    for _, group in groups:\n",
    "        if group['y_true'].sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        group_sorted = group.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        pos_idx = group_sorted.index[group_sorted['y_true'] == 1]\n",
    "        \n",
    "        for i in k:\n",
    "            top_k = group_sorted.head(i)\n",
    "            hit = top_k['y_true'].max()\n",
    "            k_dict[i].append(hit)\n",
    "        \n",
    "        for idx in pos_idx:\n",
    "            ranks.append(idx + 1)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "    curr = {\n",
    "        \"Fold\": fold,\n",
    "        \"Top_rank\": ranks.min(),\n",
    "        \"Lowest_rank\": ranks.max(),\n",
    "        \"Percentiles\": np.percentile(ranks, [10, 25, 50, 75, 90])\n",
    "    }\n",
    "    for i, hits in k_dict.items():\n",
    "        curr[f\"Top_{i}_avg_hits\"] = np.mean(hits)\n",
    "    cv_results.append(curr)\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"Top-1 Accuracy: {curr['Top_1_avg_hits']:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {curr['Top_3_avg_hits']:.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {curr['Top_5_avg_hits']:.4f}\")\n",
    "    print(f\"Top Rank: {curr['Top_rank']}\")\n",
    "    print(f\"Lowest Rank: {curr['Lowest_rank']}\")\n",
    "    \n",
    "    # Clean up memory after each fold to clear up RAM\n",
    "    del model, optimizer, criterion, train_loader, train_dataset, batch_sampler\n",
    "    del X_train, y_train, X_val, y_val\n",
    "    del X_train_features, X_val_features, X_train_scaled, X_val_scaled\n",
    "    del train_group_ids\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db95134f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Top_rank</th>\n",
       "      <th>Lowest_rank</th>\n",
       "      <th>Percentiles</th>\n",
       "      <th>Top_1_avg_hits</th>\n",
       "      <th>Top_3_avg_hits</th>\n",
       "      <th>Top_5_avg_hits</th>\n",
       "      <th>Top_10_avg_hits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[1.0, 1.0, 3.0, 6.0, 10.800000000000004]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>[1.0, 1.0, 2.0, 4.0, 6.199999999999996]</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>[1.0, 1.0, 1.0, 3.0, 7.6000000000000085]</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  Top_rank  Lowest_rank                               Percentiles  \\\n",
       "0     1         1           22  [1.0, 1.0, 3.0, 6.0, 10.800000000000004]   \n",
       "1     2         1           18   [1.0, 1.0, 2.0, 4.0, 6.199999999999996]   \n",
       "2     3         1           35  [1.0, 1.0, 1.0, 3.0, 7.6000000000000085]   \n",
       "\n",
       "   Top_1_avg_hits  Top_3_avg_hits  Top_5_avg_hits  Top_10_avg_hits  \n",
       "0        0.400000        0.625000        0.725000         0.900000  \n",
       "1        0.485714        0.771429        0.914286         0.971429  \n",
       "2        0.595238        0.857143        0.880952         0.952381  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Results\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a738e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Performance Across Folds:\n",
      "Top-1 Accuracy: 0.4936507936507937\n",
      "Top-3 Accuracy: 0.7511904761904762\n",
      "Top-5 Accuracy: 0.8400793650793651\n",
      "Top-10 Accuracy: 0.9412698412698411\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Performance Across Folds:\")\n",
    "for i in k:\n",
    "    print(f\"Top-{i} Accuracy: {cv_df[f'Top_{i}_avg_hits'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e93c2",
   "metadata": {},
   "source": [
    "Our neural network achieved an average top-1 accuracy of approximately 50% across cross-validation folds, which is competitive with Light GBM's 53% but shows significantly higher variance (ranging from 40% in Fold 1 to 60% in Fold 3). This variance suggests that certain seasons are easier to predict than others, and neural networks are more sensitive to these differences in data distribution compared to tree-based models. While Fold 3's 60% accuracy demonstrates that neural networks can match or exceed LightGBM's performance on some validation sets, the inconsistency makes LightGBM a more stable and reliable choice for deployment. Nonetheless, lets continue on to tuning our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cde4fc",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f3f10",
   "metadata": {},
   "source": [
    "Just like for Light GBM model, lets use optuna to tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d66498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_objective(trial):    \n",
    "    # Hyperparameters\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True), # Proportion of step when updating weights during gradient descent\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [128, 256, 512]), # Number of samples processed together before updating weights\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5), # Probability of randomly \"turning off neurons\" during training to prevent overfitting\n",
    "        'hidden_dim_1': trial.suggest_categorical('hidden_dim_1', [256, 512, 768]),\n",
    "        'hidden_dim_2': trial.suggest_categorical('hidden_dim_2', [128, 256, 384]),\n",
    "        'hidden_dim_3': trial.suggest_categorical('hidden_dim_3', [64, 128, 192]),\n",
    "        'hidden_dim_4': trial.suggest_categorical('hidden_dim_4', [32, 64, 96]),\n",
    "        'margin': trial.suggest_float('margin', 0.5, 2.0), # Minimum score difference required between winner and non-winner in our loss function\n",
    "        'epochs': 30, # Fixed to save time during training, decrease from 50 to 30 to decrease trial time\n",
    "    }\n",
    "    \n",
    "    hidden_dims = [params['hidden_dim_1'], params['hidden_dim_2'], \n",
    "                   params['hidden_dim_3'], params['hidden_dim_4']]\n",
    "    \n",
    "    unique_seasons = sorted(X['season'].unique())\n",
    "    folds = 2 # Decrease fold due to running multiple trials\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for fold in range(1, folds + 1):\n",
    "        val_season = unique_seasons[-fold]\n",
    "        train_seasons = unique_seasons[:-fold]\n",
    "        \n",
    "        val_mask = X['season'] == val_season\n",
    "        train_mask = X['season'].isin(train_seasons)\n",
    "        \n",
    "        X_train = X[train_mask].copy()\n",
    "        y_train = y[train_mask].copy()\n",
    "        X_val = X[val_mask].copy()\n",
    "        y_val = y[val_mask].copy()\n",
    "        \n",
    "        # Create group IDs\n",
    "        X_train['group_id'] = X_train.groupby(['season', 'week', 'conference']).ngroup()\n",
    "        train_group_ids = X_train['group_id'].values\n",
    "        \n",
    "        # Extract features\n",
    "        X_train_features = X_train.drop(columns=['group_id']).values\n",
    "        X_val_features = X_val.values\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_features).astype(np.float32)\n",
    "        X_val_scaled = scaler.transform(X_val_features).astype(np.float32)\n",
    "        \n",
    "        # Create dataset\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.from_numpy(X_train_scaled),\n",
    "            torch.from_numpy(y_train.values.astype(np.float32)),\n",
    "            torch.from_numpy(train_group_ids.astype(np.int64))\n",
    "        )\n",
    "        \n",
    "        batch_sampler = GroupBatchSampler(train_group_ids, batch_size=params['batch_size'])\n",
    "        train_loader = DataLoader(train_dataset, batch_sampler=batch_sampler)\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X_train_scaled.shape[1]\n",
    "        model = POTWRanker(input_dim, hidden_dims=hidden_dims, dropout=params['dropout'])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = PairwiseRankingLoss(margin=params['margin'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Training\n",
    "        for epoch in range(params['epochs']):\n",
    "            model.train()\n",
    "            for batch_X, batch_y, batch_groups in train_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_groups = batch_groups.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                scores = model(batch_X)\n",
    "                loss = criterion(scores, batch_y, batch_groups)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.from_numpy(X_val_scaled).to(device)\n",
    "            val_scores = model(X_val_tensor).cpu().squeeze().numpy()\n",
    "        \n",
    "        df_val = X_val[['season', 'week', 'conference']].copy().reset_index(drop=True)\n",
    "        df_val['score'] = val_scores\n",
    "        df_val['y_true'] = y_val.reset_index(drop=True).values\n",
    "        \n",
    "        # Calculate Top-1 accuracy\n",
    "        hits = []\n",
    "        for _, group in df_val.groupby(['season', 'week', 'conference'], observed=True):\n",
    "            if group['y_true'].sum() == 0:\n",
    "                continue\n",
    "            group_sorted = group.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "            hit = 1 if group_sorted.iloc[0]['y_true'] == 1 else 0\n",
    "            hits.append(hit)\n",
    "        \n",
    "        fold_accuracy = np.mean(hits)\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(fold_accuracy, fold)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model, optimizer, criterion, train_loader, train_dataset, batch_sampler\n",
    "        del X_train, y_train, X_val, y_val, train_group_ids\n",
    "        del X_train_features, X_val_features, X_train_scaled, X_val_scaled\n",
    "        gc.collect()\n",
    "    \n",
    "    # Return mean accuracy across folds\n",
    "    return np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c3bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 00:20:42,749] A new study created in memory with name: no-name-bb01eb73-f063-4b96-9431-4102d5f165c9\n",
      "Best trial: 0. Best value: 0.425:   7%|▋         | 1/15 [11:02<2:34:40, 662.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 00:31:45,640] Trial 0 finished with value: 0.42500000000000004 and parameters: {'learning_rate': 0.0005611516415334506, 'batch_size': 128, 'dropout': 0.1624074561769746, 'hidden_dim_1': 768, 'hidden_dim_2': 256, 'hidden_dim_3': 64, 'hidden_dim_4': 96, 'margin': 1.2871346474483567}. Best is trial 0 with value: 0.42500000000000004.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  13%|█▎        | 2/15 [19:39<2:04:59, 576.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 00:40:22,282] Trial 1 finished with value: 0.5053571428571428 and parameters: {'learning_rate': 0.0007309539835912913, 'batch_size': 256, 'dropout': 0.21685785941408728, 'hidden_dim_1': 768, 'hidden_dim_2': 384, 'hidden_dim_3': 128, 'hidden_dim_4': 96, 'margin': 1.7125960221746916}. Best is trial 1 with value: 0.5053571428571428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  20%|██        | 3/15 [28:04<1:48:50, 544.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 00:48:47,661] Trial 2 finished with value: 0.3875 and parameters: {'learning_rate': 0.0004066563313514797, 'batch_size': 256, 'dropout': 0.14881529393791154, 'hidden_dim_1': 768, 'hidden_dim_2': 256, 'hidden_dim_3': 128, 'hidden_dim_4': 32, 'margin': 1.8422410256414732}. Best is trial 1 with value: 0.5053571428571428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  27%|██▋       | 4/15 [46:52<2:21:59, 774.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:07:35,165] Trial 3 finished with value: 0.4946428571428571 and parameters: {'learning_rate': 0.0015696396388661144, 'batch_size': 128, 'dropout': 0.11809091556421523, 'hidden_dim_1': 512, 'hidden_dim_2': 128, 'hidden_dim_3': 192, 'hidden_dim_4': 64, 'margin': 0.7980735223012586}. Best is trial 1 with value: 0.5053571428571428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  33%|███▎      | 5/15 [57:47<2:01:52, 731.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:18:29,803] Trial 4 finished with value: 0.45357142857142857 and parameters: {'learning_rate': 0.00010257563974185662, 'batch_size': 128, 'dropout': 0.4085081386743783, 'hidden_dim_1': 512, 'hidden_dim_2': 128, 'hidden_dim_3': 192, 'hidden_dim_4': 96, 'margin': 1.2083223877429239}. Best is trial 1 with value: 0.5053571428571428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  40%|████      | 6/15 [1:05:57<1:37:25, 649.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:26:40,593] Trial 5 finished with value: 0.4821428571428571 and parameters: {'learning_rate': 0.00017345566642360953, 'batch_size': 256, 'dropout': 0.40838687198182444, 'hidden_dim_1': 512, 'hidden_dim_2': 256, 'hidden_dim_3': 64, 'hidden_dim_4': 32, 'margin': 1.633326707814573}. Best is trial 1 with value: 0.5053571428571428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  47%|████▋     | 7/15 [1:10:08<1:09:13, 519.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:30:51,400] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  53%|█████▎    | 8/15 [1:15:39<53:34, 459.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:36:22,142] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  60%|██████    | 9/15 [1:23:50<46:55, 469.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:44:33,562] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  67%|██████▋   | 10/15 [1:27:14<32:15, 387.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:47:56,997] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  73%|███████▎  | 11/15 [1:30:36<22:02, 330.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 01:51:19,354] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  80%|████████  | 12/15 [1:41:10<21:08, 422.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 02:01:53,492] Trial 11 finished with value: 0.4232142857142857 and parameters: {'learning_rate': 0.0013838713864867114, 'batch_size': 128, 'dropout': 0.2095091787743595, 'hidden_dim_1': 256, 'hidden_dim_2': 128, 'hidden_dim_3': 128, 'hidden_dim_4': 64, 'margin': 0.534169419173486}. Best is trial 1 with value: 0.5053571428571428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  87%|████████▋ | 13/15 [1:45:11<12:15, 367.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 02:05:54,286] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357:  93%|█████████▎| 14/15 [1:50:26<05:51, 351.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 02:11:09,228] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.505357: 100%|██████████| 15/15 [1:53:39<00:00, 454.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 02:14:22,113] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1), # Does not prune first 5 trials, waits after 1st fold to prune\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "# Run optimization, using 15 vs. 50 due to long trial time\n",
    "study.optimize(nn_objective, n_trials=15, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6d60f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 15\n",
      "Best trial value (Top-1 Accuracy): 0.5054\n",
      "\n",
      "Best hyperparameters:\n",
      "  learning_rate: 0.0007309539835912913\n",
      "  batch_size: 256\n",
      "  dropout: 0.21685785941408728\n",
      "  hidden_dim_1: 768\n",
      "  hidden_dim_2: 384\n",
      "  hidden_dim_3: 128\n",
      "  hidden_dim_4: 96\n",
      "  margin: 1.7125960221746916\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial value (Top-1 Accuracy): {study.best_value:.4f}\")\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf458790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Configuration:\n",
      "  Learning Rate: 0.000731\n",
      "  Batch Size: 256\n",
      "  Dropout: 0.22\n",
      "  Hidden Dims: [768, 384, 128, 96]\n",
      "  Margin: 1.71\n"
     ]
    }
   ],
   "source": [
    "# Get best params\n",
    "best_params = study.best_params\n",
    "\n",
    "# Reconstruct hidden dims\n",
    "hidden_dims_best = [\n",
    "    best_params['hidden_dim_1'],\n",
    "    best_params['hidden_dim_2'],\n",
    "    best_params['hidden_dim_3'],\n",
    "    best_params['hidden_dim_4']\n",
    "]\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"  Learning Rate: {best_params['learning_rate']:.6f}\")\n",
    "print(f\"  Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"  Dropout: {best_params['dropout']:.2f}\")\n",
    "print(f\"  Hidden Dims: {hidden_dims_best}\")\n",
    "print(f\"  Margin: {best_params['margin']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2949f",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f8c45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0011\n",
      "Epoch 20/100, Loss: 0.0011\n",
      "Epoch 30/100, Loss: 0.0011\n",
      "Epoch 40/100, Loss: 0.0011\n",
      "Epoch 50/100, Loss: 0.0011\n",
      "Epoch 60/100, Loss: 0.0011\n",
      "Epoch 70/100, Loss: 0.0011\n",
      "Epoch 80/100, Loss: 0.0011\n",
      "Epoch 90/100, Loss: 0.0011\n",
      "Epoch 100/100, Loss: 0.0011\n",
      "Final model saved\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters from optuna trials\n",
    "epochs = 50\n",
    "batch_size = best_params['batch_size']\n",
    "hidden_dims = hidden_dims_best\n",
    "dropout = best_params['dropout']\n",
    "lr = best_params['learning_rate']\n",
    "margin = best_params['margin']\n",
    "\n",
    "# Train on ALL available data (no hold-out), we have our 2025-26 season data for inference\n",
    "X_all = X.copy()  # All data from 2001-2024\n",
    "y_all = y.copy()\n",
    "\n",
    "# Sort by key\n",
    "X_all['group_id'] = X_all.groupby(['season', 'week', 'conference']).ngroup()\n",
    "all_group_ids = X_all['group_id'].values\n",
    "X_all = X_all.sort_values(by='group_id')\n",
    "y_all = y_all.loc[X_all.index]\n",
    "\n",
    "# Extract features\n",
    "feature_cols = [col for col in X_all.columns if col not in ['group_id']]\n",
    "X_all_features = X_all[feature_cols].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all_features).astype(np.float32)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_all_scaled),\n",
    "    torch.from_numpy(y_all.values.astype(np.float32)),\n",
    "    torch.from_numpy(all_group_ids.astype(np.int64))\n",
    ")\n",
    "\n",
    "batch_sampler = GroupBatchSampler(all_group_ids, batch_size=batch_size)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "# Train with best hyperparameters from Optuna\n",
    "model = POTWRanker(input_dim, hidden_dims=hidden_dims, dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = PairwiseRankingLoss(margin=margin)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for batch_X, batch_y, batch_groups in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        batch_groups = batch_groups.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scores = model(batch_X)\n",
    "        loss = criterion(scores, batch_y, batch_groups)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'nn_final_model.pth')\n",
    "torch.save(scaler, 'nn_scaler.pkl') # Save fitted scaler to transform future data\n",
    "print(\"Final model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40d10b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b17d8",
   "metadata": {},
   "source": [
    "We will now move on to testing our model on data from the current season: 2025-26. We will test our model for each week so far during this season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a9073de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing column: team_Bobcats\n",
      "Added missing column: team_SuperSonics\n",
      "Number of rows with null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Reminder: total_inf is our inference set containing data from the 2025-26 season\n",
    "# Select the same features our model was trained on\n",
    "total_inf = total_inf.reset_index(drop=True)\n",
    "X_inf = total_inf.drop(columns=['full_name', 'player_id', 'pow_player_id', 'player_of_the_week', 'won_player_of_the_week', 'all_star_this_season', 'mvp_this_season',\n",
    " 'all_nba_first_team_this_season', 'all_nba_second_team_this_season', 'all_nba_third_team_this_season', 'week_start', 'pow_conference',\n",
    " 'is_win_vs_over_500', 'opponent_has_all_nba']).reset_index(drop=True)\n",
    "\n",
    "# True results\n",
    "y_inf = total_inf['won_player_of_the_week'].reset_index(drop=True)\n",
    "\n",
    "# One-hot encode\n",
    "X_inf = pd.get_dummies(X_inf, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Rename conference one-hot encoding column: 'conference_West' -> 'conference'\n",
    "X_inf = X_inf.rename(columns={'conference_West': 'conference'})\n",
    "\n",
    "# Add missing columns (teams that don't exist anymore) with all zeros\n",
    "for col in X_all.drop(columns=['group_id']).columns:\n",
    "    if col not in X_inf.columns:\n",
    "        X_inf[col] = 0  # Add missing column with zeros\n",
    "        print(f\"Added missing column: {col}\")\n",
    "\n",
    "# Double check our data does not contain null values\n",
    "has_null_rows = X_inf.isnull().any(axis=1)\n",
    "num_rows_with_nulls = has_null_rows.sum()\n",
    "print(f\"Number of rows with null values: {num_rows_with_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a38b4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>games_played_this_week</th>\n",
       "      <th>numMinutes</th>\n",
       "      <th>points</th>\n",
       "      <th>assists</th>\n",
       "      <th>blocks</th>\n",
       "      <th>steals</th>\n",
       "      <th>reboundsTotal</th>\n",
       "      <th>reboundsDefensive</th>\n",
       "      <th>...</th>\n",
       "      <th>team_Spurs</th>\n",
       "      <th>team_Suns</th>\n",
       "      <th>team_Thunder</th>\n",
       "      <th>team_Timberwolves</th>\n",
       "      <th>team_Trail Blazers</th>\n",
       "      <th>team_Warriors</th>\n",
       "      <th>team_Wizards</th>\n",
       "      <th>conference</th>\n",
       "      <th>team_Bobcats</th>\n",
       "      <th>team_SuperSonics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>15.78</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>133.12</td>\n",
       "      <td>92.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>100.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>97.73</td>\n",
       "      <td>55.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>2025</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>2025</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>68.75</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>2025</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>128.70</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>2025</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>5.48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>2025</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1497 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season  week  games_played_this_week  numMinutes  points  assists  \\\n",
       "0       2025    44                       1        6.76     0.0      0.0   \n",
       "1       2025    44                       2       15.78     6.0      0.0   \n",
       "2       2025    44                       4      133.12    92.0     11.0   \n",
       "3       2025    44                       4      100.52    56.0     24.0   \n",
       "4       2025    44                       3       97.73    55.0     30.0   \n",
       "...      ...   ...                     ...         ...     ...      ...   \n",
       "1492    2025    48                       1        4.56     0.0      0.0   \n",
       "1493    2025    48                       3       68.75    30.0      1.0   \n",
       "1494    2025    48                       3      128.70    44.0     12.0   \n",
       "1495    2025    48                       1        5.48     3.0      0.0   \n",
       "1496    2025    48                       1        4.56     0.0      0.0   \n",
       "\n",
       "      blocks  steals  reboundsTotal  reboundsDefensive  ...  team_Spurs  \\\n",
       "0        0.0     0.0            2.0                2.0  ...       False   \n",
       "1        1.0     0.0            1.0                1.0  ...       False   \n",
       "2        2.0     2.0           21.0               21.0  ...       False   \n",
       "3        0.0     0.0           13.0               11.0  ...       False   \n",
       "4        0.0     4.0           22.0               20.0  ...       False   \n",
       "...      ...     ...            ...                ...  ...         ...   \n",
       "1492     0.0     0.0            0.0                0.0  ...       False   \n",
       "1493     0.0     4.0           14.0                9.0  ...       False   \n",
       "1494     2.0     2.0           10.0                8.0  ...       False   \n",
       "1495     0.0     0.0            0.0                0.0  ...       False   \n",
       "1496     0.0     0.0            0.0                0.0  ...       False   \n",
       "\n",
       "      team_Suns  team_Thunder  team_Timberwolves  team_Trail Blazers  \\\n",
       "0         False         False              False               False   \n",
       "1         False         False              False               False   \n",
       "2         False         False              False               False   \n",
       "3         False         False              False               False   \n",
       "4         False         False              False               False   \n",
       "...         ...           ...                ...                 ...   \n",
       "1492      False         False              False               False   \n",
       "1493      False         False              False               False   \n",
       "1494      False         False              False               False   \n",
       "1495      False         False              False               False   \n",
       "1496      False         False              False               False   \n",
       "\n",
       "      team_Warriors  team_Wizards  conference  team_Bobcats  team_SuperSonics  \n",
       "0             False         False        True             0                 0  \n",
       "1             False         False       False             0                 0  \n",
       "2             False         False       False             0                 0  \n",
       "3             False         False       False             0                 0  \n",
       "4             False         False       False             0                 0  \n",
       "...             ...           ...         ...           ...               ...  \n",
       "1492          False         False        True             0                 0  \n",
       "1493           True         False        True             0                 0  \n",
       "1494          False         False       False             0                 0  \n",
       "1495          False         False       False             0                 0  \n",
       "1496          False         False        True             0                 0  \n",
       "\n",
       "[1497 rows x 95 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd7e96a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total weeks evaluated: 8\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "X_inf_features = X_inf.values\n",
    "\n",
    "# Scale features with SAME scaler from training\n",
    "X_inf_scaled = scaler.transform(X_inf_features).astype(np.float32)\n",
    "\n",
    "# Convert to tensors and move to device\n",
    "X_inf_tensor = torch.from_numpy(X_inf_scaled).to(device)\n",
    "\n",
    "# Predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_inf_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "# Evaluate results\n",
    "results = []\n",
    "k = [1, 3, 5, 10]\n",
    "k_dict = {i: [] for i in k}\n",
    "\n",
    "# Group predictions by season, week, conference\n",
    "for (season, week, conference), group in X_inf.groupby(['season', 'week', 'conference'], observed=True):\n",
    "    idx = group.index.to_numpy()  # these are now positions 0..1496\n",
    "\n",
    "    # Use positional indexing everywhere\n",
    "    y_group = y_inf.iloc[idx].to_numpy()\n",
    "    pred_group = pred[idx]\n",
    "\n",
    "    # Skip if no winner i.e. present week\n",
    "    if y_group.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create results dataframe\n",
    "    df = total_inf.loc[group.index, ['full_name', 'team']].copy()\n",
    "    df['score'] = pred_group\n",
    "    df['y_true'] = y_group\n",
    "    \n",
    "    # Sort by prediction score\n",
    "    df_sorted = df.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Find actual winner's rank\n",
    "    winner_idx = df_sorted[df_sorted['y_true'] == 1].index\n",
    "    if len(winner_idx) > 0:\n",
    "        winner_rank = winner_idx[0] + 1  # Convert to 1-indexed\n",
    "        \n",
    "        result = {\n",
    "            'week': week,\n",
    "            'conference': conference,\n",
    "            'actual_winner': df_sorted[df_sorted['y_true'] == 1]['full_name'].values[0],\n",
    "            'predicted_winner': df_sorted.iloc[0]['full_name'],\n",
    "            'winner_rank': winner_rank,\n",
    "            'top_5': df_sorted.head(5)[['full_name', 'team', 'score', 'y_true']].to_dict('records')\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Calculate top-k accuracy\n",
    "        for i in k:\n",
    "            top_k = df_sorted.head(i)\n",
    "            hit = top_k['y_true'].max()\n",
    "            k_dict[i].append(hit)\n",
    "    \n",
    "print(f\"\\nTotal weeks evaluated: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d106fae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Week 45 - False:\n",
      "Actual: Cade Cunningham\n",
      "Predicted: Cade Cunningham\n",
      "Top 5 predictions:\n",
      "1. Cade Cunningham (Pistons) - Score: -3.2761\n",
      "2. Giannis Antetokounmpo (Bucks) - Score: -9.4162\n",
      "3. Donovan Mitchell (Cavaliers) - Score: -10.6207\n",
      "4. Tyrese Maxey (76ers) - Score: -19.6053\n",
      "5. Evan Mobley (Cavaliers) - Score: -19.8634\n",
      "\n",
      "Week 45 - True:\n",
      "Actual: Nikola Jokic\n",
      "Predicted: Shai Gilgeous-Alexander\n",
      "Top 5 predictions:\n",
      "1. Shai Gilgeous-Alexander (Thunder) - Score: -20.0971\n",
      "2. Alperen Sengun (Rockets) - Score: -20.2184\n",
      "3. Nikola Jokic (Nuggets) - Score: -22.0988\n",
      "4. Devin Booker (Suns) - Score: -47.8813\n",
      "5. Amen Thompson (Rockets) - Score: -50.5350\n",
      "\n",
      "Week 46 - False:\n",
      "Actual: Jalen Johnson\n",
      "Predicted: Scottie Barnes\n",
      "Top 5 predictions:\n",
      "1. Scottie Barnes (Raptors) - Score: -6.5365\n",
      "2. Derrick White (Celtics) - Score: -13.7224\n",
      "3. Giannis Antetokounmpo (Bucks) - Score: -17.3811\n",
      "4. Franz Wagner (Magic) - Score: -22.2874\n",
      "5. Jakob Poeltl (Raptors) - Score: -22.8969\n",
      "\n",
      "Week 46 - True:\n",
      "Actual: Nikola Jokic\n",
      "Predicted: James Harden\n",
      "Top 5 predictions:\n",
      "1. James Harden (Clippers) - Score: 0.0008\n",
      "2. Luka Doncic (Lakers) - Score: -6.6817\n",
      "3. Kevin Durant (Rockets) - Score: -16.7955\n",
      "4. Alperen Sengun (Rockets) - Score: -23.6327\n",
      "5. Nikola Jokic (Nuggets) - Score: -24.1554\n",
      "\n",
      "Week 47 - False:\n",
      "Actual: Donovan Mitchell\n",
      "Predicted: Tyrese Maxey\n",
      "Top 5 predictions:\n",
      "1. Tyrese Maxey (76ers) - Score: -0.1463\n",
      "2. Donovan Mitchell (Cavaliers) - Score: -1.7869\n",
      "3. Kel'el Ware (Heat) - Score: -5.8592\n",
      "4. Brandon Ingram (Raptors) - Score: -7.4843\n",
      "5. Jaylen Brown (Celtics) - Score: -8.1593\n",
      "\n",
      "Week 47 - True:\n",
      "Actual: Shai Gilgeous-Alexander\n",
      "Predicted: James Harden\n",
      "Top 5 predictions:\n",
      "1. James Harden (Clippers) - Score: -1.4196\n",
      "2. Shai Gilgeous-Alexander (Thunder) - Score: -8.6151\n",
      "3. Luka Doncic (Lakers) - Score: -22.7197\n",
      "4. Ivica Zubac (Clippers) - Score: -33.4797\n",
      "5. Nikola Jokic (Nuggets) - Score: -53.7476\n",
      "\n",
      "Week 48 - False:\n",
      "Actual: Jalen Brunson\n",
      "Predicted: Jaylen Brown\n",
      "Top 5 predictions:\n",
      "1. Jaylen Brown (Celtics) - Score: -2.0838\n",
      "2. Scottie Barnes (Raptors) - Score: -4.4894\n",
      "3. Tyrese Maxey (76ers) - Score: -11.8653\n",
      "4. Cade Cunningham (Pistons) - Score: -18.5006\n",
      "5. Brandon Ingram (Raptors) - Score: -20.8585\n",
      "\n",
      "Week 48 - True:\n",
      "Actual: Luka Doncic\n",
      "Predicted: Luka Doncic\n",
      "Top 5 predictions:\n",
      "1. Luka Doncic (Lakers) - Score: -4.4780\n",
      "2. Austin Reaves (Lakers) - Score: -9.9642\n",
      "3. James Harden (Clippers) - Score: -29.3983\n",
      "4. Alperen Sengun (Rockets) - Score: -40.8862\n",
      "5. Shai Gilgeous-Alexander (Thunder) - Score: -43.3114\n",
      "Overall Inference Performance:\n",
      "Top-1 Accuracy: 0.2500 (2/8 weeks)\n",
      "Top-3 Accuracy: 0.6250 (5/8 weeks)\n",
      "Top-5 Accuracy: 0.7500 (6/8 weeks)\n",
      "Top-10 Accuracy: 1.0000 (8/8 weeks)\n",
      "\n",
      "Mean Winner Rank: 3.62\n",
      "Median Winner Rank: 2.5\n",
      "Best Rank: 1\n",
      "Worst Rank: 8\n"
     ]
    }
   ],
   "source": [
    "# Show week by week peformance\n",
    "for result in results:\n",
    "    print(f\"\\nWeek {result['week']} - {result['conference']}:\")\n",
    "    print(f\"Actual: {result['actual_winner']}\")\n",
    "    print(f\"Predicted: {result['predicted_winner']}\")\n",
    "    print(f\"Top 5 predictions:\")\n",
    "    for i, player in enumerate(result['top_5'], 1):\n",
    "        print(f\"{i}. {player['full_name']} ({player['team']}) - Score: {player['score']:.4f}\")\n",
    "\n",
    "# Overall metrics\n",
    "print(\"Overall Inference Performance:\")\n",
    "\n",
    "for i in k:\n",
    "    accuracy = np.mean(k_dict[i])\n",
    "    print(f\"Top-{i} Accuracy: {accuracy:.4f} ({int(accuracy * len(results))}/{len(results)} weeks)\")\n",
    "\n",
    "ranks = [r['winner_rank'] for r in results]\n",
    "print(f\"\\nMean Winner Rank: {np.mean(ranks):.2f}\")\n",
    "print(f\"Median Winner Rank: {np.median(ranks):.1f}\")\n",
    "print(f\"Best Rank: {min(ranks)}\")\n",
    "print(f\"Worst Rank: {max(ranks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874443cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49725eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMPARISON WITH LIGHTGBM\")\n",
    "print(f\"LightGBM Top-1:        53.0%\")\n",
    "print(f\"Neural Network Top-1:  {cv_df['Top_1_avg_hits'].mean():.1%}\")\n",
    "print(f\"Difference:            {(cv_df['Top_1_avg_hits'].mean() - 0.53)*100:+.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
